{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from Bio.SeqIO import QualityIO\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import gzip\n",
    "\n",
    "from utils import dna_rev_comp, translate_dna2aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_a4 = \"MNINDLIREIKNKDYTVKLSGTDSNSITQLIIRVNNDGNEYVISESENESIVEKFISAFKNGWNQEYEDEEEFYNDMQTITLKSELN\"\n",
    "gt_a5 = \"MAYGKSRYNSYRKRSFNRSNKQRREYAQEMDRLEKAFENLDGWYLSSMKDSAYKDFGKYEIRLSNHSADNKYHDLENGRLIVNIKASKLNFVDIIENKLDKIIEKIDKLDLDKYRFINATNLEHDIKCYYKGFKTKKEVI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sequences(file_1, file_2, min_overlap=50, max_overlap=250):\n",
    "    sequences = []\n",
    "    a_sequences = []\n",
    "    b_sequences = []\n",
    "\n",
    "    with gzip.open(file_1, \"rt\") as a_file, gzip.open(file_2, \"rt\") as b_file:\n",
    "        a_reader = QualityIO.FastqGeneralIterator(a_file)\n",
    "        b_reader = QualityIO.FastqGeneralIterator(b_file)\n",
    "        for total_read, (a, b) in enumerate(zip(a_reader, b_reader)):\n",
    "            a_id, a_seq, a_qual = a\n",
    "            b_id, b_seq, b_qual = b\n",
    "            if len(a_seq) < 250:\n",
    "                a_seq = a_seq + \"G\"\n",
    "            if len(b_seq) < 250:\n",
    "                b_seq = b_seq + \"G\"\n",
    "            a_sequences.append(a_seq)\n",
    "            b_sequences.append(b_seq)\n",
    "            b_inv = dna_rev_comp(b_seq)\n",
    "            for expected_overlap in range(min_overlap, max_overlap):\n",
    "                if expected_overlap == 0 or a_seq[-expected_overlap:] == b_inv[:expected_overlap]:\n",
    "                    res_seq = a_seq + b_inv[expected_overlap:]\n",
    "                    sequences.append(res_seq)\n",
    "                    break\n",
    "        print(total_read)\n",
    "    return sequences\n",
    "\n",
    "def gather_variants(sequences, catch, gt):\n",
    "    count = 0\n",
    "    translations = {}\n",
    "    multiples = []\n",
    "    stops = []\n",
    "    mislengths = []\n",
    "    wildtype = 0\n",
    "\n",
    "    length = 3 * len(gt)\n",
    "    peptide_length = len(gt)\n",
    "    catch_length = len(catch)\n",
    "    \n",
    "    dist = np.zeros((length, 4))\n",
    "\n",
    "    for sequence in sequences:\n",
    "        tr = None\n",
    "        if catch in sequence:\n",
    "            index = sequence.index(catch) + catch_length\n",
    "            gene = sequence[index:index + length]\n",
    "            tr = translate_dna2aa(gene)\n",
    "            count += 1\n",
    "        if catch in dna_rev_comp(sequence):\n",
    "            sequence = dna_rev_comp(sequence)\n",
    "            index = sequence.index(catch) + catch_length\n",
    "            gene = sequence[index:index + length]\n",
    "            tr = translate_dna2aa(gene)\n",
    "            count += 1\n",
    "        if tr is not None:\n",
    "            if \"*\" in tr:\n",
    "                stops.append(tr)\n",
    "                continue\n",
    "            if len(tr) != peptide_length:\n",
    "                mislengths.append(tr)\n",
    "                continue\n",
    "            if tr == gt:\n",
    "                wildtype += 1\n",
    "            if (np.array([c for c in tr]) != np.array([c for c in gt])).sum() > 1:\n",
    "                multiples.append(tr)\n",
    "                continue\n",
    "            for idx, val in enumerate(gene):\n",
    "                if tr[idx // 3] != gt[idx // 3]:\n",
    "                    dist[idx, \"GATC\".index(val)] += 1\n",
    "            if tr not in translations:\n",
    "                translations[tr] = 0\n",
    "            translations[tr] += 1\n",
    "    return dist, translations, multiples, stops, mislengths, wildtype\n",
    "\n",
    "AA_CODE = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "def check_mutants(translations, gt):\n",
    "    length = len(gt)\n",
    "    result = np.zeros((length, 20))\n",
    "    for tr in translations:\n",
    "        if tr == gt:\n",
    "            for idx, c in enumerate(tr):\n",
    "                result[idx, AA_CODE.index(c)] = 1#float(\"nan\")\n",
    "        for idx, c in enumerate(tr):\n",
    "            if c != gt[idx]:\n",
    "                result[idx, AA_CODE.index(c)] += translations[tr]\n",
    "    return result\n",
    "\n",
    "def process_directory(base, catch, gt, min_overlap=50, max_overlap=250):\n",
    "    peptide_length = len(gt)\n",
    "    length = len(gt) * 3\n",
    "\n",
    "    fraction_paths_r1 = sorted([\n",
    "        f\"{base}/{path}\"\n",
    "        for path in os.listdir(base)\n",
    "        if \"R1\" in path or \"1_sequence\" in path\n",
    "    ])\n",
    "    fraction_paths_r2 = sorted([\n",
    "        f\"{base}/{path}\"\n",
    "        for path in os.listdir(base)\n",
    "        if \"R2\" in path or \"2_sequence\" in path\n",
    "    ])\n",
    "\n",
    "    sequences = []\n",
    "    results = []\n",
    "    for f1, f2 in zip(fraction_paths_r1, fraction_paths_r2):\n",
    "        seq = read_sequences(f1, f2, min_overlap=min_overlap, max_overlap=max_overlap)\n",
    "        res = gather_variants(seq, catch, gt)\n",
    "        wt = res[-1]\n",
    "        res = (check_mutants(res[1], gt), wt)\n",
    "        sequences.append(seq)\n",
    "        results.append(res)\n",
    "        \n",
    "    return sequences, results\n",
    "\n",
    "def normalise_single_run(result):\n",
    "    wt = result[-1] + 1\n",
    "    variants = result[0] + 1\n",
    "    total = wt + variants.sum()\n",
    "    wt_norm = wt / total\n",
    "    variants_norm = variants / total\n",
    "    return variants_norm, wt_norm\n",
    "\n",
    "def relative_results(normalised):\n",
    "    variant_stack = np.stack(map(lambda x: x[0], normalised))\n",
    "    wt_stack = np.array([item[-1] for item in normalised])\n",
    "    variant_stack = variant_stack / variant_stack.sum(axis=0, keepdims=True)\n",
    "    wt_stack = wt_stack / wt_stack.sum(axis=0, keepdims=True)\n",
    "    return variant_stack, wt_stack\n",
    "\n",
    "def normalise_results(results):\n",
    "    normalised = []\n",
    "    for item in results:\n",
    "        normalised.append(normalise_single_run(item))\n",
    "    relative = relative_results(normalised)\n",
    "    return normalised, relative\n",
    "\n",
    "def plot_contacts(upper, lower, positions, cutoff=None):\n",
    "    upper_ratio = sum(upper) / (sum(lower) + sum(upper))\n",
    "    gt_indices = [AA_CODE.index(gt[pos]) for pos in positions]\n",
    "    gt_matrix = np.zeros((20, len(positions)))\n",
    "    for idx, pos in enumerate(gt_indices):\n",
    "        gt_matrix[pos, idx] = 1\n",
    "\n",
    "    top = upper_ratio[positions].T / (1 - gt_matrix)\n",
    "    if cutoff:\n",
    "        top = 1.0 * (top > cutoff)\n",
    "    cmable = plt.matshow(top)\n",
    "    ax = plt.gca()\n",
    "    ax.set_yticks(range(20))\n",
    "    ax.set_yticklabels(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    ax.set_xticks(range(len(positions)))\n",
    "    ax.set_xticklabels([pos + 1 for pos in positions])\n",
    "    plt.colorbar(cmable);\n",
    "\n",
    "def selected_fraction(variants, wt, select=None):\n",
    "    select = select if isinstance(select, (list, tuple)) else [select]\n",
    "    wt_upper = sum(wt[idx] for idx in select)\n",
    "    wt_lower = sum(wt[idx] for idx in range(len(variants)) if idx not in select)\n",
    "    upper = sum(variants[idx] for idx in select)\n",
    "    lower = sum(variants[idx] for idx in range(len(variants)) if idx not in select)\n",
    "    return lower, upper, wt_lower, wt_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = \"aggaattcgcca\".upper()\n",
    "base = f\"{base_dir}/embla4ds3/\"\n",
    "\n",
    "if not os.path.isfile(f\"{base_dir}/savefile_a4_ds3.npy\"):\n",
    "    sequences, results = process_directory(base, catch, gt_a4)\n",
    "    normalised, relative = normalise_results(results)\n",
    "    variants_flat = relative[0].reshape(len(results), -1)\n",
    "    relative_a4_ds3 = relative\n",
    "    results_a4_ds3 = results\n",
    "\n",
    "    np.save(f\"{base_dir}/savefile_a4_ds3.npy\", (relative_a4_ds3, results_a4_ds3))\n",
    "else:\n",
    "    relative_a4_ds3, results_a4_ds3 = np.load(f\"{base_dir}/savefile_a4_ds3.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = \"aggaattcgcca\".upper()\n",
    "base = f\"{base_dir}/embla5ds3/\"\n",
    "\n",
    "if not os.path.isfile(f\"{base_dir}/savefile_a5_ds3.npy\"):\n",
    "    sequences, results = process_directory(base, catch, gt, min_overlap=10)\n",
    "    normalised, relative = normalise_results(results)\n",
    "    variants_flat = relative[0].reshape(len(results), -1)\n",
    "    relative_a5_ds3 = relative\n",
    "    results_a5_ds3 = results\n",
    "\n",
    "    np.save(f\"{base_dir}/savefile_a5_ds3.npy\", (relative_a5_ds3, results_a5_ds3))\n",
    "else:\n",
    "    relative_a5_ds3, results_a5_ds3 = np.load(f\"{base_dir}/savefile_a5_ds3.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_counts(data):\n",
    "    result = np.concatenate([\n",
    "        item[0][:, :, None]\n",
    "        for item in data\n",
    "    ], axis=-1)\n",
    "    total = result.sum() + sum(item[1] for item in data)\n",
    "    return np.log((result.sum(axis=2) + 1) / total)\n",
    "\n",
    "def absolute_counts(data):\n",
    "    result = np.concatenate([\n",
    "        item[0][:, :, None]\n",
    "        for item in data\n",
    "    ], axis=-1)\n",
    "    return result.sum(axis=2)\n",
    "\n",
    "def read_entropy(data):\n",
    "    return (data[0] * np.log(data[0] * 8)).sum(axis=0)\n",
    "\n",
    "raw_counts_a4_ds3 = raw_counts(results_a4_ds3)\n",
    "raw_counts_a5_ds3 = raw_counts(results_a5_ds3)\n",
    "\n",
    "absolute_counts_a4_ds3 = absolute_counts(results_a4_ds3)\n",
    "absolute_counts_a5_ds3 = absolute_counts(results_a5_ds3)\n",
    "\n",
    "entropy_a4_ds3 = read_entropy(relative_a4_ds3)\n",
    "entropy_a5_ds3 = read_entropy(relative_a5_ds3)\n",
    "\n",
    "\n",
    "plt.matshow(raw_counts_a4_ds3.T)\n",
    "plt.matshow(raw_counts_a5_ds3.T)\n",
    "\n",
    "cx = plt.matshow((raw_counts_a4_ds3).T)\n",
    "plt.colorbar(cx)\n",
    "plt.savefig(f\"{base_dir}/coverage_a4.svg\")\n",
    "cx = plt.matshow((raw_counts_a5_ds3).T)\n",
    "plt.colorbar(cx)\n",
    "plt.savefig(f\"{base_dir}/coverage_a5.svg\")\n",
    "\n",
    "plt.matshow(entropy_a4_ds3.T)\n",
    "plt.matshow(entropy_a5_ds3.T)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(raw_counts_a4_ds1.reshape(-1), -entropy_a4_ds1.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fraction of variants with N reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_counts = dict(a4=absolute_counts_a4_ds3, a5=absolute_counts_a5_ds3)\n",
    "for key in absolute_counts:\n",
    "    for N in [1, 5, 10, 50, 100]:\n",
    "        print(f\"{key} > {N}\", 100 * (absolute_counts[key] >= N).sum() / absolute_counts[key].size, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare confidence for A4 and A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import violinplot\n",
    "\n",
    "data = dict(\n",
    "    value=list(entropy_a4_ds3.reshape(-1)) + list(entropy_a5_ds3.reshape(-1)),\n",
    "    name=[\"A4\"] * len(entropy_a4_ds3.reshape(-1)) + [\"A5\"] * len(entropy_a5_ds3.reshape(-1))\n",
    ")\n",
    "\n",
    "violinplot(x=\"name\", y=\"value\", data=data, inner=\"quartiles\")\n",
    "plt.savefig(f\"{base_dir}/entropy_distribution_a4_vs_a5.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 / Bottom 10 confidence by entropy on reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range(87 * 20))\n",
    "index = sorted(index, key=lambda x: entropy_a4_ds3.reshape(-1)[x])\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 5))\n",
    "for i in range(10):\n",
    "    x, y = np.unravel_index(index[-(i + 1)], (87, 20))\n",
    "    ax[0, i].plot(relative_a4_ds3[0][:, x, y])\n",
    "    ax[0, i].set_ylim(0, 1)\n",
    "for i in range(10):\n",
    "    x, y = np.unravel_index(index[i], (87, 20))\n",
    "    ax[1, i].plot(relative_a4_ds3[0][:, x, y])\n",
    "    ax[1, i].set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 / Bottom 10 confidence by read fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range(87 * 20))\n",
    "index = sorted(index, key=lambda x: raw_counts_a4_ds3.reshape(-1)[x])\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 5))\n",
    "for i in range(10):\n",
    "    x, y = np.unravel_index(index[-(i + 1)], (87, 20))\n",
    "    ax[0, i].plot(relative_a4_ds3[0][:, x, y])\n",
    "    ax[0, i].set_ylim(0, 1)\n",
    "for i in range(10):\n",
    "    x, y = np.unravel_index(index[i], (87, 20))\n",
    "    ax[1, i].plot(relative_a4_ds3[0][:, x, y])\n",
    "    ax[1, i].set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit FACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FlowCal\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.distributions import MixtureSameFamily, Normal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Constant(torch.nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, out_size, requires_grad=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.bias.expand(inputs.size(0), self.bias.size(1))\n",
    "\n",
    "def fit_facs(relative, path, frac=8, start=4, end=12, factor=2, steps=100000, mode=\"linear\", drop=None):\n",
    "    if mode == \"linear\":\n",
    "        linear = torch.nn.Linear(frac, (end - start) * factor + 1, bias=True)\n",
    "        with torch.no_grad():\n",
    "            linear.weight.zero_()\n",
    "            linear.bias.zero_()\n",
    "    else: # mode == \"constant\"\n",
    "        linear = Constant((end - start) * factor + 1)\n",
    "    loss = torch.nn.KLDivLoss(reduction=\"mean\")\n",
    "    optimizer = torch.optim.AdamW(linear.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    targets = []\n",
    "    vals = []\n",
    "    for p in os.listdir(path):\n",
    "        kind = p.split(\".\")[0].split(\"_\")[3]\n",
    "        original = kind[0]\n",
    "        changed = kind[-1]\n",
    "        position = int(kind[1:-1])\n",
    "        data = FlowCal.io.FCSData(f\"{path}/{p}\")\n",
    "        target = np.log(data[:, 6][data[:, 6] >= 1])\n",
    "        binned = (2 * (torch.tensor(target).clamp(start, end) - start)).floor().long()\n",
    "        target = torch.zeros((end - start) * factor + 1)\n",
    "        unique, counts = binned.unique(return_counts=True)\n",
    "        target[unique] = 1.0 * counts\n",
    "        target = target / target.sum()\n",
    "        targets.append(target[None])\n",
    "        changed = AA_CODE.index(changed)\n",
    "        vals.append(torch.tensor(relative[0][:, position - 1, changed])[None])\n",
    "    vals = torch.cat(vals, dim=0).float()\n",
    "    targets = torch.cat(targets, dim=0).float()\n",
    "\n",
    "    indices = [idx for idx in range(len(targets))]\n",
    "    if drop is not None:\n",
    "        del indices[drop]\n",
    "    indices = torch.tensor(indices)\n",
    "    targets = targets[indices]\n",
    "    vals = vals[indices]\n",
    "    for idx in range(steps):\n",
    "        v = vals\n",
    "        t = targets\n",
    "        out = linear(v)\n",
    "        out = out.softmax(dim=1)\n",
    "        val = ((out - t) ** 2).sum()\n",
    "        print(float(val), end=\"\\r\")\n",
    "        val.backward()\n",
    "        optimizer.step()\n",
    "    return linear\n",
    "\n",
    "def eval_facs(linears, relative, path, frac=8, start=4, end=12, factor=2, steps=100000, lin=True, drop=None):\n",
    "    targets = []\n",
    "    vals = []\n",
    "    for p in os.listdir(path):\n",
    "        kind = p.split(\".\")[0].split(\"_\")[3]\n",
    "        original = kind[0]\n",
    "        changed = kind[-1]\n",
    "        position = int(kind[1:-1])\n",
    "        data = FlowCal.io.FCSData(f\"{path}/{p}\")\n",
    "        target = np.log(data[:, 6][data[:, 6] >= 1])\n",
    "        binned = (2 * (torch.tensor(target).clamp(start, end) - start)).floor().long()\n",
    "        target = torch.zeros((end - start) * factor + 1)\n",
    "        unique, counts = binned.unique(return_counts=True)\n",
    "        target[unique] = 1.0 * counts\n",
    "        target = target / target.sum()\n",
    "        targets.append(target[None])\n",
    "        changed = AA_CODE.index(changed)\n",
    "        vals.append(torch.tensor(relative[0][:, position - 1, changed])[None])\n",
    "    vals = torch.cat(vals, dim=0).float()\n",
    "    targets = torch.cat(targets, dim=0).float()\n",
    "\n",
    "    errors = []\n",
    "    predictions = []\n",
    "    for idx, reg in enumerate(linears):\n",
    "        errors.append(((reg(vals)[idx].softmax(dim=0) - targets[idx]) ** 2).mean())\n",
    "        predictions.append(reg(vals)[idx].softmax(dim=0).detach())\n",
    "    return errors, predictions, targets\n",
    "        \n",
    "def predict_facs(linear, data, frac=8):\n",
    "    inputs = torch.tensor(data[0]).float().permute(1, 2, 0)\n",
    "    pred = linear(inputs.view(-1, frac))\n",
    "    pred = pred.view(*inputs.shape[:2], pred.size(1))\n",
    "    return pred\n",
    "\n",
    "def predict_wt(linear, data, frac=8):\n",
    "    inputs = torch.tensor(data[1]).float()[None]\n",
    "    pred = linear(inputs.view(-1, frac))\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_a4_ds3 = fit_facs(relative_a4_ds3, f\"{base_dir}/FACS/FACS/A4/\", mode=\"linear\", start=0, end=12, drop=None)\n",
    "linear_a5_ds3 = fit_facs(relative_a5_ds3, f\"{base_dir}/FACS/FACS/A5_new/\", mode=\"linear\", frac=8, start=0, end=12, drop=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit two fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_a4_ds3_f2 = (\n",
    "    np.concatenate((\n",
    "        relative_a4_ds3[0][:4].sum(axis=0, keepdims=True),\n",
    "        relative_a4_ds3[0][4:].sum(axis=0, keepdims=True)\n",
    "    ), axis=0),\n",
    "    np.concatenate((\n",
    "        relative_a4_ds3[1][:4].sum(axis=0, keepdims=True),\n",
    "        relative_a4_ds3[1][4:].sum(axis=0, keepdims=True)\n",
    "    ), axis=0),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_a4_ds3_f2 = fit_facs(\n",
    "    relative_a4_ds3_f2,\n",
    "    f\"{base_dir}/FACS/FACS/A4/\",\n",
    "    mode=\"linear\", frac=2, start=0, end=12, drop=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linears_a4_ds3 = []\n",
    "for idx in range(16):\n",
    "    linears_a4_ds3.append(fit_facs(relative_a4_ds3, f\"{base_dir}/FACS/FACS/A4/\", mode=\"linear\", start=0, end=12, drop=idx))\n",
    "linears_a5_ds3 = []\n",
    "for idx in range(16):\n",
    "    linears_a5_ds3.append(fit_facs(relative_a5_ds3, f\"{base_dir}/FACS/FACS/A5_new/\", mode=\"linear\", start=0, end=12, drop=idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### two fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linears_a4_ds3_f2 = []\n",
    "for idx in range(16):\n",
    "    linears_a4_ds3_f2.append(fit_facs(\n",
    "        relative_a4_ds3_f2,\n",
    "        f\"{base_dir}/FACS/FACS/A4/\",\n",
    "        mode=\"linear\", frac=2, start=0, end=12, drop=idx\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if not os.path.isfile(f\"{base_dir}/backup.torch\"):\n",
    "    torch.save(dict(\n",
    "        linear_a4_ds3=linear_a4_ds3,\n",
    "        linear_a5_ds3=linear_a5_ds3,\n",
    "        linears_a4_ds3=linears_a4_ds3,\n",
    "        linears_a5_ds3=linears_a5_ds3\n",
    "    ), f\"{base_dir}/backup.torch\")\n",
    "else:\n",
    "    backup = torch.load(f\"{base_dir}/backup.torch\")\n",
    "    linear_a4_ds3 = backup[\"linear_a4_ds3\"]\n",
    "    linear_a5_ds3 = backup[\"linear_a5_ds3\"]\n",
    "    linears_a4_ds3 = backup[\"linears_a4_ds3\"]\n",
    "    linears_a5_ds3 = backup[\"linears_a5_ds3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_a4, cpredictions_a4, ctargets_a4 = eval_facs(linears_a4_ds3, relative_a4_ds3, f\"{base_dir}/FACS/FACS/A4/\", factor=2, lin=True, start=0, end=12)\n",
    "errors_a4_f2, cpredictions_a4_f2, ctargets_a4_f2 = eval_facs(linears_a4_ds3_f2, relative_a4_ds3_f2, f\"{base_dir}/FACS/FACS/A4/\", factor=2, frac=2, lin=True, start=0, end=12)\n",
    "errors_a5, cpredictions_a5, ctargets_a5 = eval_facs(linears_a5_ds3, relative_a5_ds3, f\"{base_dir}/FACS/FACS/A5_new/\", factor=2, lin=True, start=0, end=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot cross-validation error for variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_names_a4 = []\n",
    "for p in os.listdir(f\"{base_dir}/FACS/FACS/A4/\"):\n",
    "        kind = p.split(\"_\")[3]\n",
    "        original = kind[0]\n",
    "        changed = kind[-1]\n",
    "        variant_names_a4.append(kind)\n",
    "variant_names_a5 = []\n",
    "for p in os.listdir(f\"{base_dir}/FACS/FACS/A5_new/\"):\n",
    "        kind = p[:-4].split(\"_\")[3]\n",
    "        original = kind[0]\n",
    "        changed = kind[-1]\n",
    "        variant_names_a5.append(kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_names = dict(\n",
    "    a4=variant_names_a4, a5=variant_names_a5, a4_f2=variant_names_a4\n",
    ")\n",
    "ctargets = dict(\n",
    "    a4=ctargets_a4, a5=ctargets_a5, a4_f2=ctargets_a4_f2\n",
    ")\n",
    "cpredictions = dict(\n",
    "    a4=cpredictions_a4, a5=cpredictions_a5, a4_f2=cpredictions_a4_f2\n",
    ")\n",
    "errors = dict(\n",
    "    a4=errors_a4, a5=errors_a5, a4_f2=errors_a4_f2\n",
    ")\n",
    "for acrkind in (\"a4\", \"a5\", \"a4_f2\"):\n",
    "    for idx, name in enumerate(variant_names[acrkind]):\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.set_title(name)\n",
    "        ax.fill_between(range(25), ctargets[acrkind][idx], alpha=0.5)\n",
    "        ax.fill_between(range(25), cpredictions[acrkind][idx], alpha=0.5)\n",
    "        ax.set_ylim(0.0, 0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{base_dir}/error_{acrkind}_{name}.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot mean cross-validation error across all variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acrkind in (\"a4\", \"a5\", \"a4_f2\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.bar(list(range(len(errors[acrkind]))), [e.detach() for e in errors[acrkind]])\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.set_xticks(range(16))\n",
    "    ax.set_xticklabels(variant_names[acrkind])\n",
    "    ax.set_ylim(0.0, 0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_dir}/leave_one_out_{acrkind}.svg\")\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(list(range(len(errors[acrkind]))), [(en - e).detach() for e, en in zip(errors_a5, errors_a5_no0)])\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.set_xticks(range(16))\n",
    "ax.set_xticklabels(variant_names[acrkind])\n",
    "# ax.set_ylim(0.0, 0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_dir}/leave_one_out_a5_comparison.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict FACS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = predict_facs(linear_a4_ds3.eval(), relative_a4_ds3)\n",
    "pf_wt = predict_wt(linear_a4_ds3.eval(), relative_a4_ds3)\n",
    "pf_rel = pf.softmax(dim=-1).detach().numpy()\n",
    "pf_wt = pf_wt.softmax(dim=0).detach().numpy()\n",
    "pf_esc = (abs(pf) / abs(pf).sum(dim=-1, keepdim=True)).detach().numpy()\n",
    "pf_wt_a4 = pf_wt\n",
    "pf_rel_a4 = pf_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation tolerance A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_fluorescence = (pf_rel * np.arange(25)[None, None, :] * 0.5).sum(axis=-1)\n",
    "mean_log_fluorescence_wt = (pf_wt * np.arange(25) * 0.5).sum(axis=-1)\n",
    "for threshold in [0.9, 0.95, 1.0]:\n",
    "    print(threshold, (mean_log_fluorescence >= mean_log_fluorescence_wt * threshold).sum() / mean_log_fluorescence.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation with Kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations\n",
    "kd_variants = dict(WT=8.31, G38C=14.8, N25G=8.69, E70T=112, E70D=24.5, Y67K=332000, M77A=76)\n",
    "mean_bin_variants = dict(WT=(pf_wt_a4 * np.arange(len(pf_wt_a4))).sum())\n",
    "for name in kd_variants:\n",
    "    if name not in mean_bin_variants:\n",
    "        pos = int(name[1:-1]) - 1\n",
    "        val = AA_CODE.index(name[-1])\n",
    "        pf_variant = pf_rel_a4[pos, val]\n",
    "        print(pf_variant.shape)\n",
    "        mean_bin_variants[name] = (pf_variant * np.arange(len(pf_wt))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_names = sorted(list(kd_variants.keys()))\n",
    "kd_variants_rv = [1 / kd_variants[name] for name in sorted_names]\n",
    "mean_bin_variants_v = [mean_bin_variants[name] for name in sorted_names]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(mean_bin_variants_v, kd_variants_rv)\n",
    "for i, txt in enumerate(sorted_names):\n",
    "    ax.annotate(txt, (mean_bin_variants_v[i], kd_variants_rv[i]))\n",
    "ax.set_xlabel(\"mean predicted bin\")\n",
    "ax.set_ylabel(\"1 / Kd\")\n",
    "plt.savefig(f\"{base_dir}/Kd_correlation_a4.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def dot_heatmap(predictions, predictions_wt, confidence, gt, path, scale=0.25):\n",
    "    pf_rel = predictions\n",
    "    pf_wt = predictions_wt\n",
    "    pf_corr = pf_rel - pf_wt[None, None]#pf_rel.mean(axis=(0, 1), keepdims=True)\n",
    "    pf_corr = pf_corr * np.array(range(pf_corr.shape[-1]))[None, None, :]\n",
    "    pf_mean = pf_corr.sum(axis=-1) / 2#pf_corr.shape[-1]\n",
    "    wt = (pf_wt * np.array(range(pf_corr.shape[-1]))).sum() / 2\n",
    "    print(wt)\n",
    "    #pf_mean = pf_mean + 0.5\n",
    "    pmax = -pf_mean.min()\n",
    "    pmax = max(pmax, pf_mean.max())\n",
    "    pmin = -pmax\n",
    "    pmax = pmax + wt\n",
    "    pmin = pmin + wt\n",
    "    #pf_mean = pf_mean / pmax\n",
    "\n",
    "    x = 0.25\n",
    "    width = pf_rel.shape[0]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width * x, 20 * x), sharex=False, sharey=False)\n",
    "\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    c = []\n",
    "    s = []\n",
    "    for idy in range(20):\n",
    "        for idx in range(width):\n",
    "            data_x.append(idx)\n",
    "            data_y.append(-idy)\n",
    "            vals = pf_rel[idx, idy]\n",
    "            mean = pf_mean[idx, idy] + wt\n",
    "            if AA_CODE.index(gt[idx]) == idy:\n",
    "                c.append(100)\n",
    "                s.append(1)\n",
    "            else:\n",
    "                c.append(mean)\n",
    "                s.append(confidence[idx, idy] / 2)\n",
    "                #s.append((vals * np.log(vals * 8)).sum())\n",
    "\n",
    "    #print(min(s), max(s))\n",
    "    s = [\n",
    "        0.99 * (x - min(s)) / (max(s) - min(s)) + 0.01\n",
    "        for x in s\n",
    "    ]\n",
    "    cmap = cm.bwr\n",
    "\n",
    "    ca = plt.scatter(data_x, data_y, c=c, s=[sv * 100 for sv in s], cmap=cmap, vmin=pmin, vmax=pmax)\n",
    "    ca.cmap.set_over(\"black\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar(ca)\n",
    "    fig.savefig(path)\n",
    "\n",
    "dot_heatmap(pf_rel_a4, pf_wt_a4, entropy_a4_ds3, gt_a4, f\"{base_dir}/large-prediction-dotheatmap-A4-DS1-final.svg\", scale=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### amino acid type overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position-wise violin plots\n",
    "cas_positions = [13, 16, 17, 35, 37, 38, 39, 66, 68, 69]\n",
    "non_cas_positions = [idx for idx in range(87) if idx not in cas_positions]\n",
    "surface_positions = [0, 1, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 78, 79, 80, 81] # TODO\n",
    "core_positions = [5, 9, 16, 29, 31, 33, 50, 54, 58, 72, 76]\n",
    "polar = \"RNDCEQHKSTWY\"\n",
    "non_polar = \"AGILMFPV\"\n",
    "basic = \"RHK\"\n",
    "acidic = \"DE\"\n",
    "neutral = \"ANCQGILMFPSTWYV\"\n",
    "polar_positions = [idx for idx, c in enumerate(gt_a4) if c in polar] # TODO\n",
    "nonpolar_positions = [idx for idx, c in enumerate(gt_a4) if c in non_polar]\n",
    "acidic_positions = [idx for idx, c in enumerate(gt_a4) if c in acidic]\n",
    "basic_positions = [idx for idx, c in enumerate(gt_a4) if c in basic]\n",
    "neutral_positions = [idx for idx, c in enumerate(gt_a4) if c in neutral]\n",
    "helix_positions = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 50, 51, 52, 53, 54, 55, 56, 57, 58, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n",
    "sheet_positions = [28, 29, 30, 31, 32, 39, 40, 41, 42, 43]\n",
    "loop_positions = [0, 1, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 33, 34, 35, 36, 37, 38, 44, 45, 46, 47, 48, 49, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_positions = {}\n",
    "names = []\n",
    "values = []\n",
    "for name, pos in {\n",
    "    \"spyCas9 contact\": cas_positions,\n",
    "    \"non-contact\": non_cas_positions,\n",
    "    \"surface\": surface_positions,\n",
    "    \"core\": core_positions,\n",
    "    \"polar\": polar_positions,\n",
    "    \"non-polar\": nonpolar_positions,\n",
    "    \"acidic\": acidic_positions,\n",
    "    \"basic\": basic_positions,\n",
    "    \"neutral\": neutral_positions,\n",
    "    \"structured\": helix_positions + sheet_positions,\n",
    "    \"loop\": loop_positions\n",
    "}.items():\n",
    "    items = []\n",
    "    positions = pos\n",
    "    for pos in positions:\n",
    "        update = list((pf_rel_a4[pos] * 0.5 * (np.arange(25)[None, :] + 1)).sum(axis=1) - (pf_wt_a4 * (0.5 * np.arange(25) + 0.5)).sum(axis=0))\n",
    "        update = [item for idx, item in enumerate(update) if entropy_a4_ds3[pos, idx] >= 0.0]\n",
    "        items += update\n",
    "    names.extend([name] * len(items))\n",
    "    values.extend(items)\n",
    "\n",
    "split_positions = dict(name=names, value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import violinplot\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "violinplot(x=\"name\", y=\"value\", width=1, data=split_positions, ax=ax, inner=\"quartiles\")\n",
    "\n",
    "count = 0\n",
    "for l in ax.lines:\n",
    "    if count % 3 == 1:\n",
    "        l.set_linestyle('-')\n",
    "        l.set_color('black')\n",
    "    else:\n",
    "        l.set_linestyle(\":\")\n",
    "        l.set_color('black')\n",
    "    count += 1\n",
    "\n",
    "plt.savefig(f\"{base_dir}/acriia4_activity_distribution.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big overview plot (sequencing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(87, 20, figsize=(40, 180))\n",
    "for idx in range(87):\n",
    "    for idy in range(20):\n",
    "        ax[idx, idy].plot(relative_a4_ds3[0][:, idx, idy])\n",
    "        ax[idx, idy].set_ylim(0, 0.4)\n",
    "        ax[idx, idy].set_xticks(range(8))\n",
    "        ax[idx, idy].set_xticklabels([])\n",
    "        ax[idx, idy].set_title(f\"{idx + 1}: {gt[idx]}{idx + 1}{AA_CODE[idy]}\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{base_dir}/large-overview-reads-A4-DS3.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big overview plot (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(87, 20, figsize=(40, 180))\n",
    "for idx in range(87):\n",
    "    for idy in range(20):\n",
    "        ax[idx, idy].plot(pf_rel_a4[idx, idy])\n",
    "        ax[idx, idy].set_ylim(0, 0.5)\n",
    "        ax[idx, idy].set_xticklabels([])\n",
    "        ax[idx, idy].set_title(f\"{idx + 1}: {gt[idx]}{idx + 1}{AA_CODE[idy]}\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{base_dir}/large-overview-prediction-A4.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict KL divergence (trRosetta / Alphafold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{base_dir}/trRosetta/acrin/a4/a4.a3m\", \"w\") as f:\n",
    "    f.write(f\">a4\\n\")\n",
    "    f.write(gt + \"\\n\")\n",
    "for idx in range(len(gt_a4)):\n",
    "    for a1 in AA_CODE:\n",
    "        seq = [c for c in gt_a4]\n",
    "        seq[idx] = a1\n",
    "        seq = \"\".join(seq)\n",
    "        if seq != gt:\n",
    "            name = f\"a4{gt[idx]}{idx}{seq[idx]}\"\n",
    "            with open(f\"{base_dir}/trRosetta/acrin/a4/{name}.a3m\", \"w\") as f:\n",
    "                f.write(f\">{name}\\n\")\n",
    "                f.write(seq + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_CODE = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "kl_map = torch.zeros(20, 87) * float(\"nan\")\n",
    "rkl_map = torch.zeros(20, 87) * float(\"nan\")\n",
    "base = f\"{base_dir}/alpha-dms/\"\n",
    "wt = torch.tensor(np.load(f\"{base_dir}/alpha-dms/DMSjob_wt_model_1.npy\"))\n",
    "log_wt = wt.log_softmax(dim=2)\n",
    "for fname in os.listdir(base):\n",
    "    if fname.endswith(\".npy\"):\n",
    "        if \"wt\" in fname:\n",
    "            continue\n",
    "        key = fname.split(\"_\")[1]\n",
    "        pos = int(key[1:-1])\n",
    "        aa = AA_CODE.index(key[-1])\n",
    "        data = torch.tensor(np.load(f\"{base}{fname}\"))\n",
    "        log_p = data.log_softmax(dim=2)\n",
    "        p = log_p.exp()\n",
    "        kl_div = (p * (log_p - log_wt)).sum(dim=2).sum()\n",
    "        rkl_div = (log_wt.exp() * (log_wt - log_p)).sum(dim=2).sum()\n",
    "        kl_map[aa, pos] = kl_div\n",
    "        rkl_map[aa, pos] = rkl_div\n",
    "        \n",
    "with open(f\"{base_dir}/trRosetta/a4_trRosetta_kl_divergence.csv\") as csv:\n",
    "    rosetta_kl = np.zeros((20, 87))\n",
    "    for idx, line in enumerate(csv):\n",
    "        ln = np.array([float(item) for item in line.strip().split(\",\")])\n",
    "        rosetta_kl[:, idx] = ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def kl_dot_heatmap(kl_map, predictions, predictions_wt, gt, path, scale=0.25):\n",
    "    pf_rel = predictions\n",
    "    pf_wt = predictions_wt\n",
    "    pf_corr = pf_rel - pf_wt[None, None]\n",
    "    pf_corr = pf_corr * np.array(range(pf_corr.shape[-1]))[None, None, :]\n",
    "    pf_mean = pf_corr.sum(axis=-1) / 2\n",
    "    wt = (pf_wt * np.array(range(pf_corr.shape[-1]))).sum() / 2\n",
    "    pmax = -pf_mean.min()\n",
    "    pmax = max(pmax, pf_mean.max())\n",
    "    pmin = -pmax\n",
    "    pmax = pmax + wt\n",
    "    pmin = pmin + wt\n",
    "\n",
    "    x = 0.25\n",
    "    width = pf_rel.shape[0]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width * x, 20 * x), sharex=False, sharey=False)\n",
    "\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    c = []\n",
    "    s = []\n",
    "    for idy in range(20):\n",
    "        for idx in range(width):\n",
    "            data_x.append(idx)\n",
    "            data_y.append(-idy)\n",
    "            vals = pf_rel[idx, idy]\n",
    "            mean = pf_mean[idx, idy] + wt\n",
    "            alpha = kl_map[idy, idx]\n",
    "            if AA_CODE.index(gt[idx]) == idy:\n",
    "                c.append(alpha)\n",
    "                s.append(1)\n",
    "            else:\n",
    "                c.append(alpha)\n",
    "                s.append(1)\n",
    "\n",
    "    s = [\n",
    "        0.99 * (x - min(s)) / (max(s) - min(s) + 1) + 0.01\n",
    "        for x in s\n",
    "    ]\n",
    "    cmap = cm.Greys\n",
    "\n",
    "    ca = plt.scatter(data_x, data_y, c=c, s=[sv * 5000 for sv in s], cmap=cmap)\n",
    "    ca.cmap.set_over(\"black\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar(ca)\n",
    "    fig.savefig(path)\n",
    "\n",
    "kl_dot_heatmap(rosetta_kl, pf_rel_a4, pf_wt_a4, gt_a4, f\"{base_dir}/large-prediction-dotheatmap-A4-DS1-rosetta.svg\", scale=4)\n",
    "kl_dot_heatmap(kl_map, pf_rel_a4, pf_wt_a4, gt_a4, f\"{base_dir}/large-prediction-dotheatmap-A4-DS1-alpha.svg\", scale=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export mean KL for pymol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_colors(x, cmap):\n",
    "    mean = np.nan_to_num(np.array(x), 0.0).mean(axis=0)\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    return list(map(lambda v: (cmap((v - mean.min()) / (mean.max() - mean.min())))[:3], mean))\n",
    "print(get_mean_colors(rosetta_kl, \"Greys\"))\n",
    "print(get_mean_colors(kl_map, \"Greys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_cbar(x, cmap, path):\n",
    "    mean = np.nan_to_num(np.array(x), 0.0).mean(axis=0)\n",
    "    cx = plt.matshow(mean[None], cmap=cmap)\n",
    "    plt.colorbar(cx)\n",
    "    plt.savefig(path)\n",
    "    \n",
    "get_mean_cbar(rosetta_kl, \"Greys\", f\"{base_dir}/cbar_rosetta_kl.svg\")\n",
    "get_mean_cbar(kl_map, \"Greys\", f\"{base_dir}/cbar_alpha_kl.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scatter alpha vs rosetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kl_map.reshape(-1).log(), np.log(rosetta_kl.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scatter KL vs predicted activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_scatter(kl_map, predictions, predictions_wt, gt, path, scale=0.25):\n",
    "    pf_rel = predictions\n",
    "    pf_wt = predictions_wt\n",
    "    pf_corr = pf_rel - pf_wt[None, None]#pf_rel.mean(axis=(0, 1), keepdims=True)\n",
    "    pf_corr = pf_corr * np.array(range(pf_corr.shape[-1]))[None, None, :]\n",
    "    pf_mean = pf_corr.sum(axis=-1) / 2#pf_corr.shape[-1]\n",
    "    wt = (pf_wt * np.array(range(pf_corr.shape[-1]))).sum() / 2\n",
    "    print(wt)\n",
    "    #pf_mean = pf_mean + 0.5\n",
    "    pmax = -pf_mean.min()\n",
    "    pmax = max(pmax, pf_mean.max())\n",
    "    pmin = -pmax\n",
    "    pmax = pmax + wt\n",
    "    pmin = pmin + wt\n",
    "    #pf_mean = pf_mean / pmax\n",
    "\n",
    "    x = 0.25\n",
    "    width = pf_rel.shape[0]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width * x, 20 * x), sharex=False, sharey=False)\n",
    "\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    c = []\n",
    "    s = []\n",
    "    for idy in range(20):\n",
    "        for idx in range(width):\n",
    "            data_x.append(idx)\n",
    "            data_y.append(-idy)\n",
    "            vals = pf_rel[idx, idy]\n",
    "            mean = pf_mean[idx, idy] + wt\n",
    "            alpha = kl_map[idy, idx]\n",
    "            c.append(alpha)\n",
    "            s.append(mean)\n",
    "\n",
    "    plt.scatter(c, s)\n",
    "    fig.savefig(path)\n",
    "\n",
    "kl_scatter(np.log(kl_map), pf_rel_a4, pf_wt_a4, gt_a4, f\"{base_dir}/kl-mean-scatter.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean KL vs mean activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_scatter(kl_map, predictions, predictions_wt, gt, path, scale=0.25):\n",
    "    pf_rel = predictions\n",
    "    pf_wt = predictions_wt\n",
    "    kl_nonan = np.nan_to_num(kl_map, 0.0)\n",
    "    pf_corr = pf_rel - pf_wt[None, None]#pf_rel.mean(axis=(0, 1), keepdims=True)\n",
    "    pf_corr = pf_corr * np.array(range(pf_corr.shape[-1]))[None, None, :]\n",
    "    pf_mean = pf_corr.sum(axis=-1) / 2#pf_corr.shape[-1]\n",
    "    wt = (pf_wt * np.array(range(pf_corr.shape[-1]))).sum() / 2\n",
    "    pmax = -pf_mean.min()\n",
    "    pmax = max(pmax, pf_mean.max())\n",
    "    pmin = -pmax\n",
    "    pmax = pmax + wt\n",
    "    pmin = pmin + wt\n",
    "\n",
    "    x = 0.25\n",
    "    width = pf_rel.shape[0]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width * x, 20 * x), sharex=False, sharey=False)\n",
    "\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    c = []\n",
    "    s = []\n",
    "    for idx in range(width):\n",
    "        data_x.append(idx)\n",
    "        vals = pf_rel[idx, :]\n",
    "        mean = pf_mean[idx, :]\n",
    "        alpha = kl_nonan[:, idx]\n",
    "        c.append(np.log(alpha.mean(axis=0)))\n",
    "        s.append(mean.mean(axis=0))\n",
    "\n",
    "    plt.scatter(c, s)\n",
    "    fig.savefig(path)\n",
    "\n",
    "kl_scatter(kl_map, pf_rel_a4, pf_wt_a4, gt_a4, f\"{base_dir}/kl-mean-scatter.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL by amino-acid position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position-wise violin plots\n",
    "cas_positions = [13, 16, 17, 35, 37, 38, 39, 66, 68, 69]\n",
    "non_cas_positions = [idx for idx in range(87) if idx not in cas_positions]\n",
    "surface_positions = [0, 1, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 78, 79, 80, 81] # TODO\n",
    "core_positions = [5, 9, 16, 29, 31, 33, 50, 54, 58, 72, 76]\n",
    "polar = \"RNDCEQHKSTWY\"\n",
    "non_polar = \"AGILMFPV\"\n",
    "basic = \"RHK\"\n",
    "acidic = \"DE\"\n",
    "neutral = \"ANCQGILMFPSTWYV\"\n",
    "polar_positions = [idx for idx, c in enumerate(gt_a4) if c in polar] # TODO\n",
    "nonpolar_positions = [idx for idx, c in enumerate(gt_a4) if c in non_polar]\n",
    "acidic_positions = [idx for idx, c in enumerate(gt_a4) if c in acidic]\n",
    "basic_positions = [idx for idx, c in enumerate(gt_a4) if c in basic]\n",
    "neutral_positions = [idx for idx, c in enumerate(gt_a4) if c in neutral]\n",
    "helix_positions = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 50, 51, 52, 53, 54, 55, 56, 57, 58, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n",
    "sheet_positions = [28, 29, 30, 31, 32, 39, 40, 41, 42, 43]\n",
    "loop_positions = [0, 1, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 33, 34, 35, 36, 37, 38, 44, 45, 46, 47, 48, 49, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nonpolar_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl_violin(kl):\n",
    "    split_positions = {}\n",
    "    names = []\n",
    "    values = []\n",
    "    for name, pos in {\n",
    "        \"spyCas9 contact\": cas_positions,\n",
    "        \"non-contact\": non_cas_positions,\n",
    "        \"surface\": surface_positions,\n",
    "        \"core\": core_positions,\n",
    "        \"polar\": polar_positions,\n",
    "        \"non-polar\": nonpolar_positions,\n",
    "        \"acidic\": acidic_positions,\n",
    "        \"basic\": basic_positions,\n",
    "        \"neutral\": neutral_positions,\n",
    "        \"structured\": helix_positions + sheet_positions,\n",
    "        \"loop\": loop_positions\n",
    "    }.items():\n",
    "        items = []\n",
    "        positions = pos\n",
    "        for pos in positions:\n",
    "            items += list(map(float, -np.log(kl[:, pos][kl[:, pos] > 0.0])))\n",
    "        names.extend([name] * len(items))\n",
    "        values.extend(items)\n",
    "\n",
    "    split_positions = dict(name=names, value=values)\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    violinplot(x=\"name\", y=\"value\", width=1, data=split_positions, ax=ax, inner=\"quartiles\")\n",
    "\n",
    "    count = 0\n",
    "    for l in ax.lines:\n",
    "        if count % 3 == 1:\n",
    "            l.set_linestyle('-')\n",
    "            l.set_color('black')\n",
    "        else:\n",
    "            l.set_linestyle(\":\")\n",
    "            l.set_color('black')\n",
    "        count += 1\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kl_violin(np.nan_to_num(kl_map, 0.0))\n",
    "plt.savefig(f\"{base_dir}/acriia4_alpha_kl_distribution.svg\")\n",
    "plot_kl_violin(np.nan_to_num(rosetta_kl, 0.0))\n",
    "plt.savefig(f\"{base_dir}/acriia4_tr_kl_distribution.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scatter KL vs activity for AA type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "    \"spyCas9 contact\": cas_positions,\n",
    "    \"non-contact\": non_cas_positions,\n",
    "    \"surface\": surface_positions,\n",
    "    \"core\": core_positions,\n",
    "    \"polar\": polar_positions,\n",
    "    \"non-polar\": nonpolar_positions,\n",
    "    \"acidic\": acidic_positions,\n",
    "    \"basic\": basic_positions,\n",
    "    \"neutral\": neutral_positions,\n",
    "    \"structured\": helix_positions + sheet_positions,\n",
    "    \"loop\": loop_positions\n",
    "}\n",
    "\n",
    "def plot_kl_scatter(kl, res, select=None):\n",
    "    split_positions = {}\n",
    "    names = []\n",
    "    kl_values = []\n",
    "    fluorescence = []\n",
    "    pos_dict = {\n",
    "        \"spyCas9 contact\": cas_positions,\n",
    "        \"non-contact\": non_cas_positions,\n",
    "        \"surface\": surface_positions,\n",
    "        \"core\": core_positions,\n",
    "        \"polar\": polar_positions,\n",
    "        \"non-polar\": nonpolar_positions,\n",
    "        \"acidic\": acidic_positions,\n",
    "        \"basic\": basic_positions,\n",
    "        \"neutral\": neutral_positions,\n",
    "        \"structured\": helix_positions + sheet_positions,\n",
    "        \"loop\": loop_positions\n",
    "    }\n",
    "    if select is not None:\n",
    "        pos_dict = {\n",
    "            key: pos_dict[key]\n",
    "            for key in select\n",
    "        }\n",
    "    for name, pos in pos_dict.items():\n",
    "        kl_items = []\n",
    "        fl_items = []\n",
    "        positions = pos\n",
    "        for pos in positions:\n",
    "            kl_items += list(map(float, -np.log(kl[:, pos][kl[:, pos] > 0.0])))\n",
    "            fl_items += list((res[pos, :, :] * np.arange(25)[None, :] * 0.5).sum(axis=-1)[kl[:, pos] > 0.0])\n",
    "        names.extend([name] * len(kl_items))\n",
    "        kl_values.extend(kl_items)\n",
    "        fluorescence.extend(fl_items)\n",
    "        \n",
    "    split_positions = dict(name=names, value=kl_values)\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.scatter(kl_values, fluorescence, c=[list(pos_dict.keys()).index(name) for name in names], cmap=\"tab10\")\n",
    "    return ax\n",
    "for pos in pos_dict:\n",
    "    plot_kl_scatter(kl_map, pf_rel_a4, select=[pos])\n",
    "    filepos = pos.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    plt.savefig(f\"f\"{base_dir}/alphafold_scatter_{filepos}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import violinplot, boxplot\n",
    "\n",
    "positions = []\n",
    "values = []\n",
    "for position in range(87):\n",
    "    positions += 20 * [position + 1]\n",
    "    values += list(map(float, -kl_map[:, position].log()))\n",
    "sp = dict(name=positions, value=values)\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "boxplot(x=\"name\", y=\"value\", width=1, data=sp, ax=ax)\n",
    "\n",
    "count = 0\n",
    "for l in ax.lines:\n",
    "    if count % 3 == 1:\n",
    "        l.set_linestyle('-')\n",
    "        l.set_color('black')\n",
    "    else:\n",
    "        l.set_linestyle(\":\")\n",
    "        l.set_color('black')\n",
    "    count += 1\n",
    "\n",
    "plt.savefig(f\"{base_dir}/acriia4_alpha_kl_distribution_per_position.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation mean fluorescence + cell culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_culture = dict(\n",
    "    wt=(0.958145856666667, 0.55904086),\n",
    "    K18M=(0.82442277, 0.40304539),\n",
    "    G21Q=(0.80654383, 0.32298049),\n",
    "    S24K=(0.773889983333333, 0.375188933333333),\n",
    "    S24P=(0.706929726666667, 0.30560112),\n",
    "    N25G=(0.820051383333333, 0.376600463333333),\n",
    "    I31Q=(0.83000416, 0.386209923333333),\n",
    "    E40I=(0.731740993333333, 0.33765131),\n",
    "    E70T=(0.069560613333333, 0.035723183333333),\n",
    "    M77A=(0.174492683333333, 0.084132156666667)\n",
    ")\n",
    "\n",
    "kind = 0\n",
    "names = [\"wt\"]\n",
    "cc = [cell_culture[\"wt\"][kind]]\n",
    "value = [(pf_wt_a4 * np.arange(25) * 0.5).sum()]\n",
    "for name in cell_culture:\n",
    "    if name != \"wt\":\n",
    "        pos = int(name[1:-1]) - 1\n",
    "        aa = AA_CODE.index(name[-1])\n",
    "        val = (pf_rel_a4[pos, aa] * np.arange(25) * 0.5).sum()\n",
    "        value.append(val)\n",
    "        cc.append(cell_culture[name][kind])\n",
    "        names.append(name)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(value, cc)\n",
    "\n",
    "for i, txt in enumerate(names):\n",
    "    ax.annotate(txt, (value[i], cc[i]))\n",
    "    \n",
    "plt.savefig(f\"{base_dir}/acriia4_fluorescence_vs_cell_culture.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation mean fluorescence to prior work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_activity_basgall = [0.027, 0.074, 0.015, 0.015, 0.084, 0.344, 0.845, 0.020, 0.000, 0.015, 0.113, 0.885, 0.983, 0.000, 0.595, 0.000, 0.432]\n",
    "activity_basgall = [-np.log(v + 1e-3) for v in drive_activity_basgall]\n",
    "names_basgall = [\"D14A\", \"D23R\", \"N36A\", \"D37A\", \"G38A\", \"N39A\", \"N39R\", \"E40A\", \"N48A\", \"D69A\", \"D69R\", \"E70A\", \"E70R\", \"E72A\", \"F73A\", \"D76A\", \"M77A\"]\n",
    "activity_dong = [1.067, 0.216, 0.043, 0.062, 0.056, 0.048, 0.038, 0.130, 0.022]\n",
    "names_dong = [\"N12T\", \"D14R\", \"D23R\", \"N36Y\", \"G38A\", \"N39R\", \"E40R\", \"D69R\", \"E70R\"]\n",
    "activity_dong = [np.log(v) for v in activity_dong]\n",
    "\n",
    "pred_log_basgall = [\n",
    "    (pf_rel[int(name[1:-1]) - 1, AA_CODE.index(name[-1])] * np.arange(25) * 0.5).sum()\n",
    "    for name in names_basgall\n",
    "]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].scatter(pred_log_basgall, activity_basgall)\n",
    "\n",
    "for i, txt in enumerate(names_basgall):\n",
    "    ax[0].annotate(txt, (pred_log_basgall[i], activity_basgall[i]))\n",
    "\n",
    "\n",
    "pred_log_dong = [\n",
    "    (pf_rel[int(name[1:-1]) - 1, AA_CODE.index(name[-1])] * np.arange(25) * 0.5).sum()\n",
    "    for name in names_dong\n",
    "]\n",
    "ax[1].scatter(pred_log_dong, activity_dong)\n",
    "\n",
    "for i, txt in enumerate(names_dong):\n",
    "    ax[1].annotate(txt, (pred_log_dong[i], activity_dong[i]))\n",
    "\n",
    "plt.savefig(f\"{base_dir}/correlation_prior_work.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R^2 for log-log fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 Basgall et al.\", np.corrcoef(np.array(pred_log_basgall), np.array(activity_basgall))[0, 1] ** 2)\n",
    "print(\"R^2 Dong et al.\", np.corrcoef(np.array(pred_log_dong), np.array(activity_dong))[0, 1] ** 2)\n",
    "print(\"R Basgall et al.\", np.corrcoef(np.array(pred_log_basgall), np.array(activity_basgall))[0, 1])\n",
    "print(\"R Dong et al.\", np.corrcoef(np.array(pred_log_dong), np.array(activity_dong))[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a5 spy\n",
    "pf = predict_facs(linear_a5_ds3.eval(), relative_a5_ds3)\n",
    "pf_wt = predict_wt(linear_a5_ds3.eval(), relative_a5_ds3)\n",
    "pf_wt = pf_wt.softmax(dim=0).detach().numpy()\n",
    "pf_rel = pf.softmax(dim=-1).detach().numpy()\n",
    "pf_esc = (abs(pf) / abs(pf).sum(dim=-1, keepdim=True)).detach().numpy()\n",
    "pf_wt_a5 = pf_wt\n",
    "pf_rel_a5 = pf_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation tolerance A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_fluorescence = (pf_rel * np.arange(25)[None, None, :] * 0.5).sum(axis=-1)\n",
    "mean_log_fluorescence_wt = (pf_wt * np.arange(25) * 0.5).sum(axis=-1)\n",
    "for threshold in [0.9, 0.95, 1.0]:\n",
    "    print(threshold, (mean_log_fluorescence >= mean_log_fluorescence_wt * threshold).sum() / mean_log_fluorescence.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_heatmap(pf_rel_a5, pf_wt_a5, entropy_a5_ds3, gt_a5, f\"{base_dir}/large-prediction-dotheatmap-A5-DS3.svg\", scale=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overview reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(gt_a5), 20, figsize=(40, 180))\n",
    "for idx in range(len(gt_a5)):\n",
    "    for idy in range(20):\n",
    "        ax[idx, idy].plot(relative_a5_ds3[0][:, idx, idy])\n",
    "        ax[idx, idy].set_ylim(0, 0.4)\n",
    "        ax[idx, idy].set_xticks(range(8))\n",
    "        ax[idx, idy].set_xticklabels([])\n",
    "        ax[idx, idy].set_title(f\"{idx + 1}: {gt[idx]}{idx + 1}{AA_CODE[idy]}\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{base_dir}/large-overview-reads-A5-DS3.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overview prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(gt_a5), 20, figsize=(40, 180))\n",
    "for idx in range(len(gt_a5)):\n",
    "    for idy in range(20):\n",
    "        ax[idx, idy].plot(pf_rel_a5[idx, idy])\n",
    "        ax[idx, idy].set_ylim(0, 0.5)\n",
    "        ax[idx, idy].set_xticklabels([])\n",
    "        ax[idx, idy].set_title(f\"{idx + 1}: {gt_a5[idx]}{idx + 1}{AA_CODE[idy]}\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{base_dir}/large-overview-prediction-A5.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### amino-acid type overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position-wise violin plots\n",
    "surface_positions = [0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52, 54, 55, 56, 57, 58, 59, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 89, 92, 95, 96, 99, 100, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 122, 123, 124, 126, 127, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139]\n",
    "core_positions = [3, 7, 29, 32, 35, 36, 39, 50, 51, 53, 60, 62, 63, 80, 83, 84, 88, 90, 91, 93, 94, 97, 98, 101, 105, 116, 121, 125, 128, 131]\n",
    "polar = \"RNDCEQHKSTWY\"\n",
    "non_polar = \"AGILMFPV\"\n",
    "basic = \"RHK\"\n",
    "acidic = \"DE\"\n",
    "neutral = \"ANCQGILMFPSTWYV\"\n",
    "polar_positions = [idx for idx, c in enumerate(gt_a5) if c in polar]\n",
    "nonpolar_positions = [idx for idx, c in enumerate(gt_a5) if c in non_polar]\n",
    "acidic_positions = [idx for idx, c in enumerate(gt_a5) if c in acidic]\n",
    "basic_positions = [idx for idx, c in enumerate(gt_a5) if c in basic]\n",
    "neutral_positions = [idx for idx, c in enumerate(gt_a5) if c in neutral]\n",
    "helix_positions = [4, 5, 6, 7, 8, 9, 10, 11, 12, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 135, 136, 137, 138]\n",
    "sheet_positions = [42, 43, 44, 50, 51, 52, 53, 54, 59, 60, 61, 62, 63]\n",
    "loop_positions = [0, 1, 2, 3, 13, 14, 15, 16, 17, 18, 39, 40, 41, 45, 46, 47, 48, 49, 55, 56, 57, 58, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 109, 115, 116, 117, 131, 132, 133, 134, 139]\n",
    "idr_positions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_positions = {}\n",
    "names = []\n",
    "values = []\n",
    "for name, pos in {\n",
    "    \"surface\": surface_positions,\n",
    "    \"core\": core_positions,\n",
    "    \"polar\": polar_positions,\n",
    "    \"non-polar\": nonpolar_positions,\n",
    "    \"acidic\": acidic_positions,\n",
    "    \"basic\": basic_positions,\n",
    "    \"neutral\": neutral_positions,\n",
    "    \"structured\": helix_positions + sheet_positions,\n",
    "    \"loop\": loop_positions,\n",
    "    \"idr\": idr_positions\n",
    "}.items():\n",
    "    items = []\n",
    "    positions = pos\n",
    "    for pos in positions:\n",
    "        update = list((pf_rel_a5[pos] * 0.5 * (np.arange(25)[None, :] + 1)).sum(axis=1) - (pf_wt_a5 * (0.5 * np.arange(25) + 0.5)).sum(axis=0))\n",
    "        update = [item for idx, item in enumerate(update) if entropy_a5_ds3[pos, idx] > 0.6]\n",
    "        items += update\n",
    "    names.extend([name] * len(items))\n",
    "    values.extend(items)\n",
    "\n",
    "split_positions = dict(name=names, value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import violinplot\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "violinplot(x=\"name\", y=\"value\", width=1, data=split_positions, ax=ax, inner=\"quartiles\")\n",
    "\n",
    "count = 0\n",
    "for l in ax.lines:\n",
    "    if count % 3 == 1:\n",
    "        l.set_linestyle('-')\n",
    "        l.set_color('black')\n",
    "    else:\n",
    "        l.set_linestyle(\":\")\n",
    "        l.set_color('black')\n",
    "    count += 1\n",
    "\n",
    "plt.savefig(f\"{base_dir}/acriia5_activity_distribution.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
